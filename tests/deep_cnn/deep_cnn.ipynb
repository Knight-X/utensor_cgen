{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T14:18:18.722370Z",
     "start_time": "2018-08-11T14:18:18.713958Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T14:18:23.481655Z",
     "start_time": "2018-08-11T14:18:19.346213Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.tools.graph_transforms import TransformGraph\n",
    "\n",
    "from utensor_cgen.utils import prepare_meta_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T14:18:23.495387Z",
     "start_time": "2018-08-11T14:18:23.484875Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T14:18:23.543090Z",
     "start_time": "2018-08-11T14:18:23.499102Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('cnn_weights.pkl', 'rb') as fid:\n",
    "    pretrain_weights = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T14:18:23.774598Z",
     "start_time": "2018-08-11T14:18:23.763396Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T14:18:29.166289Z",
     "start_time": "2018-08-11T14:18:29.149684Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_conv_filter(width, height, in_channels, out_channels,\n",
    "                    dtype=tf.float32, initializer=None, seed=None, name=None):\n",
    "    \"\"\"\n",
    "    arguments\n",
    "    =========\n",
    "    - width: int, filter width\n",
    "    - height: int, filter height\n",
    "    - in_channels: int, input channel\n",
    "    - out_channels: int, output channel\n",
    "    - dtype: data type\n",
    "    - initializer: filter initializer\n",
    "    - seed: random seed of the initializer\n",
    "    \"\"\"\n",
    "    if initializer is None:\n",
    "        initializer = tf.glorot_uniform_initializer(seed=seed, dtype=dtype)\n",
    "    filter_shape = [width, height, in_channels, out_channels]\n",
    "    return tf.Variable(initializer(shape=filter_shape), name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T14:18:37.930133Z",
     "start_time": "2018-08-11T14:18:37.913825Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_bias(shape, dtype=tf.float32, name=None, initializer=None, seed=None):\n",
    "    if initializer is None:\n",
    "        initializer = tf.glorot_uniform_initializer(seed=seed, dtype=dtype)\n",
    "    return tf.Variable(initializer(shape=shape), name=name, dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T14:25:40.900334Z",
     "start_time": "2018-08-11T14:25:40.878602Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_layer(in_fmap, w_shape, padding='SAME', stride=1, act_fun=None, name=None):\n",
    "    width, height, in_channel, out_channel = w_shape\n",
    "    strides = [1, stride, stride, 1]\n",
    "    with tf.name_scope(name, 'conv'):\n",
    "        w_filter = get_conv_filter(width, height, in_channel, out_channel)\n",
    "        out_fmap = tf.nn.conv2d(in_fmap, w_filter, \n",
    "                                padding=padding, \n",
    "                                strides=strides,\n",
    "                                name='feature_map')\n",
    "        bias = get_bias(w_filter.shape.as_list()[-1:],\n",
    "                        dtype=in_fmap.dtype,\n",
    "                        name='bias')\n",
    "        act = tf.add(out_fmap, bias, name='logits')\n",
    "        if act_fun:\n",
    "            act = act_fun(act, name='activation')\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T14:39:13.408404Z",
     "start_time": "2018-08-11T14:39:13.391506Z"
    }
   },
   "outputs": [],
   "source": [
    "def fc_layer(in_tensor, out_dim, act_fun=None, initializer=None, name=None):\n",
    "    \"\"\"Fully conneted layer\n",
    "    \"\"\"\n",
    "    if initializer is None:\n",
    "        initializer = tf.glorot_normal_initializer(dtype=in_tensor.dtype)\n",
    "    w_shape = [in_tensor.shape.as_list()[-1], out_dim]\n",
    "    with tf.name_scope(name, 'fully_connect'):\n",
    "        w_fc = tf.Variable(initializer(shape=w_shape, dtype=in_tensor.dtype), name='weight')\n",
    "        act = tf.matmul(in_tensor, w_fc, name='logits')\n",
    "        if act_fun:\n",
    "            act = act_fun(act, name='activation')\n",
    "    return act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T14:25:41.568871Z",
     "start_time": "2018-08-11T14:25:41.550930Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels, name=None, axis=-1):\n",
    "    '''https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3171\n",
    "    '''\n",
    "    with tf.name_scope(name, 'cross_entropy'):\n",
    "        prob = tf.nn.softmax(logits=logits, axis=axis)\n",
    "        prob = tf.clip_by_value(prob, 1e-7, 1-1e-7)\n",
    "        loss = tf.reduce_sum(-labels * tf.log(prob), name='total_loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T15:36:48.189400Z",
     "start_time": "2018-08-11T15:36:46.688526Z"
    }
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_image_batch = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "    tf_labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    tf_keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    \n",
    "    conv1 = conv_layer(tf_image_batch, [2, 2, 3, 16],\n",
    "                       padding='VALID')\n",
    "    conv2 = conv_layer(conv1,\n",
    "                       [3, 3, 16, 32],\n",
    "                       padding='VALID',\n",
    "                       act_fun=tf.nn.relu)\n",
    "    pool1 = tf.nn.max_pool(conv2,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='VALID')\n",
    "    conv3 = conv_layer(pool1,\n",
    "                       [3, 3, 32, 32],\n",
    "                       stride=2,\n",
    "                       padding='VALID')\n",
    "    conv4 = conv_layer(conv3,\n",
    "                       [3, 3, 32, 32],\n",
    "                       padding='VALID',\n",
    "                       stride=2,\n",
    "                       act_fun=tf.nn.relu)\n",
    "    drop1 = tf.nn.dropout(conv4, keep_prob=tf_keep_prob)\n",
    "    pool2 = tf.nn.max_pool(drop1,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='VALID')\n",
    "    conv5 = conv_layer(pool2,\n",
    "                       [1, 1, 32, 64],\n",
    "                       padding='VALID',\n",
    "                       act_fun=tf.nn.relu)\n",
    "    conv6 = conv_layer(conv5,\n",
    "                       [1, 1, 64, 128],\n",
    "                       act_fun=tf.nn.relu)\n",
    "    flat_conv6 = tf.reshape(conv6, shape=[-1, reduce(lambda x, y: x*y, conv6.shape.as_list()[1:], 1)])\n",
    "    fc1 = fc_layer(flat_conv6, 128, act_fun=tf.nn.relu)\n",
    "    drop_2 = tf.nn.dropout(fc1, keep_prob=tf_keep_prob)\n",
    "    fc2 = fc_layer(drop_2, 64, act_fun=tf.nn.relu)\n",
    "    logits = fc_layer(fc2, 10)\n",
    "    tf_pred = tf.argmax(logits, axis=-1, name='pred')\n",
    "    total_loss = cross_entropy_loss(logits=logits, labels=tf_labels)\n",
    "    \n",
    "    train_op = tf.train.AdadeltaOptimizer(learning_rate=1.0, epsilon=1e-7).minimize(total_loss)\n",
    "    saver = tf.train.Saver(max_to_keep=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T15:36:49.480776Z",
     "start_time": "2018-08-11T15:36:49.466199Z"
    }
   },
   "outputs": [],
   "source": [
    "from cifar import read_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T15:36:49.945291Z",
     "start_time": "2018-08-11T15:36:49.939798Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T15:36:50.346772Z",
     "start_time": "2018-08-11T15:36:50.337700Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T15:53:39.127171Z",
     "start_time": "2018-08-11T15:53:39.115280Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "num_iter_per_epoch = 1500\n",
    "num_epoch = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T17:24:26.761950Z",
     "start_time": "2018-08-11T16:02:43.432175Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf ckpt && mkdir -p ckpt/cnn\n",
    "\n",
    "# this will takes long to complete if running on CPU\n",
    "cifar = read_data_sets('./data', one_hot=True, reshape=False)\n",
    "img_gen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             horizontal_flip=True)\n",
    "img_gen.fit(cifar.train.images)\n",
    "batch_gen = img_gen.flow(cifar.train.images,\n",
    "                         cifar.train.labels,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # compute original loss\n",
    "    l, p_labels = sess.run([total_loss, tf_pred],\n",
    "                           feed_dict={tf_image_batch: cifar.test.images,\n",
    "                                      tf_labels: cifar.test.labels,\n",
    "                                      tf_keep_prob: 1.0})\n",
    "    l /= cifar.test.images.shape[0]\n",
    "    acc = (p_labels == np.argmax(cifar.test.labels, axis=-1)).mean()\n",
    "    print(f'original loss: {l}')\n",
    "    print(f'acc on test set: {acc*100:.2f}%')\n",
    "    \n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(num_epoch):\n",
    "        print(f'epoch {epoch} start')\n",
    "        for _ in range(num_iter_per_epoch):\n",
    "            images_batch, labels_batch = next(batch_gen)\n",
    "            _ = sess.run(train_op,\n",
    "                         feed_dict={tf_image_batch: images_batch,\n",
    "                                    tf_labels: labels_batch,\n",
    "                                    tf_keep_prob: 0.9})\n",
    "        test_loss, p_labels = sess.run([total_loss, tf_pred],\n",
    "                                       feed_dict={tf_image_batch: cifar.test.images,\n",
    "                                                  tf_labels: cifar.test.labels,\n",
    "                                                  tf_keep_prob: 1.0})\n",
    "        test_loss /= cifar.test.images.shape[0]\n",
    "        acc = (p_labels == np.argmax(cifar.test.labels, axis=-1)).mean()\n",
    "        print(f'test loss: {test_loss}, {acc*100:0.2f}%')\n",
    "        ckpt = saver.save(sess, 'ckpt/cnn/model', global_step=epoch)\n",
    "        if test_loss < best_loss:\n",
    "            best_loss = test_loss\n",
    "            best_ckpt = ckpt\n",
    "        print(f'epoch saved {ckpt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T17:24:26.816351Z",
     "start_time": "2018-08-11T17:24:26.801871Z"
    }
   },
   "outputs": [],
   "source": [
    "best_ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T17:24:27.081511Z",
     "start_time": "2018-08-11T17:24:26.820178Z"
    }
   },
   "outputs": [],
   "source": [
    "!tree ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T17:28:47.469952Z",
     "start_time": "2018-08-11T17:28:46.383686Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_def = prepare_meta_graph(best_ckpt+'.meta', output_nodes=[tf_pred.op.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-11T17:28:48.054678Z",
     "start_time": "2018-08-11T17:28:48.047511Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('cifar10_cnn.pb', 'wb') as fid:\n",
    "    fid.write(graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utensor",
   "language": "python",
   "name": "utensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
