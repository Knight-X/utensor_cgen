{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/vrakesh/CIFAR-10-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:02.667123Z",
     "start_time": "2018-08-10T02:08:02.662408Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:04.498922Z",
     "start_time": "2018-08-10T02:08:03.023187Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.tools.graph_transforms import TransformGraph\n",
    "\n",
    "from utensor_cgen.utils import prepare_meta_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:04.516812Z",
     "start_time": "2018-08-10T02:08:04.502549Z"
    }
   },
   "outputs": [],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:04.543333Z",
     "start_time": "2018-08-10T02:08:04.520375Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('cnn_weights.pkl', 'rb') as fid:\n",
    "    pretrain_weights = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:05.093183Z",
     "start_time": "2018-08-10T02:08:05.088971Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:05.504387Z",
     "start_time": "2018-08-10T02:08:05.497605Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_layer(in_fmap, kernel, bias, act_fun=None, name=None):\n",
    "    with tf.name_scope(name, 'conv'):\n",
    "        tf_kernel = tf.Variable(kernel,\n",
    "                                dtype=tf.float32,\n",
    "                                name='kernel')\n",
    "        tf_bias = tf.Variable(bias,\n",
    "                              dtype=tf.float32,\n",
    "                              name='bias')\n",
    "        out_fmap = tf.add(tf.nn.conv2d(in_fmap, kernel,\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding='SAME'),\n",
    "                          tf_bias,\n",
    "                          name='fmap')\n",
    "        if act_fun:\n",
    "            out_fmap = act_fun(out_fmap, name='activation')\n",
    "        return out_fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:06.651391Z",
     "start_time": "2018-08-10T02:08:06.643576Z"
    }
   },
   "outputs": [],
   "source": [
    "def fc_layer(in_fmap, weight, bias, act_fun=None, name=None):\n",
    "    with tf.name_scope(name, 'fc'):\n",
    "        tf_weight = tf.Variable(weight,\n",
    "                                dtype=tf.float32,\n",
    "                                name='weight')\n",
    "        tf_bias = tf.Variable(bias,\n",
    "                              dtype=tf.float32,\n",
    "                              name='bias')\n",
    "        logits = tf.add(tf.matmul(in_fmap, tf_weight),\n",
    "                        tf_bias,\n",
    "                        name='logits')\n",
    "        if act_fun:\n",
    "            logits = act_fun(logits, name='activation')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:07.189568Z",
     "start_time": "2018-08-10T02:08:07.181978Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels, name=None, axis=-1):\n",
    "    '''https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3171\n",
    "    '''\n",
    "    with tf.name_scope(name, 'cross_entropy'):\n",
    "        prob = tf.nn.softmax(logits=logits, axis=axis)\n",
    "        prob = tf.clip_by_value(prob, 1e-7, 1-1e-7)\n",
    "        loss = tf.reduce_sum(-labels * tf.log(prob), name='total_loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:59:03.017945Z",
     "start_time": "2018-08-10T02:59:02.122128Z"
    }
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_image_batch = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "    tf_labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    \n",
    "    conv1 = conv_layer(tf_image_batch,\n",
    "                       pretrain_weights['conv2d']['conv2d/kernel:0'],\n",
    "                       pretrain_weights['conv2d']['conv2d/bias:0'],\n",
    "                       act_fun=tf.nn.relu,\n",
    "                       name='conv1')\n",
    "    conv2 = conv_layer(conv1,\n",
    "                       pretrain_weights['conv2d_1']['conv2d_1/kernel:0'],\n",
    "                       pretrain_weights['conv2d_1']['conv2d_1/bias:0'],\n",
    "                       act_fun=tf.nn.relu,\n",
    "                       name='conv2')\n",
    "    pool1 = tf.nn.max_pool(conv2,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='VALID')\n",
    "    conv3 = conv_layer(pool1,\n",
    "                       pretrain_weights['conv2d_2']['conv2d_2/kernel:0'],\n",
    "                       pretrain_weights['conv2d_2']['conv2d_2/bias:0'],\n",
    "                       act_fun=tf.nn.relu)\n",
    "    pool2 = tf.nn.max_pool(conv3,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='VALID')\n",
    "    conv4 = conv_layer(pool2,\n",
    "                       pretrain_weights['conv2d_3']['conv2d_3/kernel:0'],\n",
    "                       pretrain_weights['conv2d_3']['conv2d_3/bias:0'],\n",
    "                       act_fun=tf.nn.relu)\n",
    "    pool3 = tf.nn.max_pool(conv4,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='VALID')\n",
    "    flat_pool3 = tf.reshape(pool3,\n",
    "                            [-1, reduce(lambda x, y: x*y,\n",
    "                                        pool3.shape.as_list()[1:],\n",
    "                                        1)])\n",
    "    tf_keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    dropout1 = tf.nn.dropout(flat_pool3,\n",
    "                             keep_prob=tf_keep_prob,\n",
    "                             name='dropout1')\n",
    "    fc1 = fc_layer(dropout1,\n",
    "                   pretrain_weights['dense']['dense/kernel:0'],\n",
    "                   pretrain_weights['dense']['dense/bias:0'],\n",
    "                   act_fun=tf.nn.relu,\n",
    "                   name='fc1')\n",
    "    dropout2 = tf.nn.dropout(fc1,\n",
    "                             keep_prob=tf_keep_prob,\n",
    "                             name='dropout2')\n",
    "    logits = fc_layer(dropout2,\n",
    "                      pretrain_weights['dense_1']['dense_1/kernel:0'],\n",
    "                      pretrain_weights['dense_1']['dense_1/bias:0'],\n",
    "                      name='logits')\n",
    "    pred_labels = tf.argmax(logits,\n",
    "                            axis=-1,\n",
    "                            name='pred')\n",
    "    total_loss = cross_entropy_loss(logits=logits, labels=tf_labels)\n",
    "    \n",
    "    train_op = tf.train.AdadeltaOptimizer(learning_rate=1.0, epsilon=1e-7).minimize(total_loss)\n",
    "    saver = tf.train.Saver(max_to_keep=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:16.143057Z",
     "start_time": "2018-08-10T02:08:09.919394Z"
    }
   },
   "outputs": [],
   "source": [
    "from cifar import read_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:16.157341Z",
     "start_time": "2018-08-10T02:08:16.152336Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:16.166038Z",
     "start_time": "2018-08-10T02:08:16.161160Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:08:16.188825Z",
     "start_time": "2018-08-10T02:08:16.169965Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_iter_per_epoch = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:49:02.259903Z",
     "start_time": "2018-08-10T02:22:11.953303Z"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf ckpt && mkdir -p ckpt/cnn\n",
    "\n",
    "# this will takes long to complete if running on CPU\n",
    "cifar = read_data_sets('./data', one_hot=True, reshape=False)\n",
    "img_gen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             horizontal_flip=True)\n",
    "img_gen.fit(cifar.train.images)\n",
    "batch_gen = img_gen.flow(cifar.train.images,\n",
    "                         cifar.train.labels,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    # compute original loss\n",
    "    l, p_labels = sess.run([total_loss, pred_labels],\n",
    "                           feed_dict={tf_image_batch: cifar.test.images,\n",
    "                                      tf_labels: cifar.test.labels,\n",
    "                                      tf_keep_prob: 1.0})\n",
    "    l /= cifar.test.images.shape[0]\n",
    "    acc = (p_labels == np.argmax(cifar.test.labels, axis=-1)).mean()\n",
    "    print(f'original loss: {l}')\n",
    "    print(f'acc on test set: {acc*100:.2f}%')\n",
    "    \n",
    "    for epoch in range(5):\n",
    "        print(f'epoch {epoch} start')\n",
    "        for _ in range(num_iter_per_epoch):\n",
    "            images_batch, labels_batch = next(batch_gen)\n",
    "            _ = sess.run(train_op,\n",
    "                         feed_dict={tf_image_batch: images_batch,\n",
    "                                    tf_labels: labels_batch,\n",
    "                                    tf_keep_prob: 0.8})\n",
    "        test_loss, p_labels = sess.run([total_loss, pred_labels],\n",
    "                                       feed_dict={tf_image_batch: cifar.test.images,\n",
    "                                                   tf_labels: cifar.test.labels,\n",
    "                                                   tf_keep_prob: 1.0})\n",
    "        test_loss /= cifar.test.images.shape[0]\n",
    "        acc = (p_labels == np.argmax(cifar.test.labels, axis=-1)).mean()\n",
    "        print(f'test loss: {test_loss}, {acc*100:0.2f}%')\n",
    "        ckpt = saver.save(sess, 'ckpt/cnn/model', global_step=epoch)\n",
    "        print(f'epoch saved {ckpt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:56:47.682112Z",
     "start_time": "2018-08-10T02:56:47.675909Z"
    }
   },
   "outputs": [],
   "source": [
    "ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:56:49.992401Z",
     "start_time": "2018-08-10T02:56:49.826989Z"
    }
   },
   "outputs": [],
   "source": [
    "!tree ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:58:32.681030Z",
     "start_time": "2018-08-10T02:58:32.425069Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_def = prepare_meta_graph(ckpt+'.meta', output_nodes=[pred_labels.op.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-10T02:58:34.198589Z",
     "start_time": "2018-08-10T02:58:34.185478Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('cifar10_cnn.pb', 'wb') as fid:\n",
    "    fid.write(graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utensor_cgen",
   "language": "python",
   "name": "utensor_cgen"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
