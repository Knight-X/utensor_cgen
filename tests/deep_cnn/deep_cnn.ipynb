{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/vrakesh/CIFAR-10-Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:13:30.031814Z",
     "start_time": "2018-08-09T13:13:30.013458Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:13:35.898237Z",
     "start_time": "2018-08-09T13:13:30.921566Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.tools.graph_transforms import TransformGraph\n",
    "\n",
    "from utensor_cgen.utils import prepare_meta_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:13:35.918536Z",
     "start_time": "2018-08-09T13:13:35.901373Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:13:38.217904Z",
     "start_time": "2018-08-09T13:13:38.133767Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('cnn_weights.pkl', 'rb') as fid:\n",
    "    pretrain_weights = pickle.load(fid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:13:38.958127Z",
     "start_time": "2018-08-09T13:13:38.930643Z"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:13:40.008791Z",
     "start_time": "2018-08-09T13:13:39.966277Z"
    }
   },
   "outputs": [],
   "source": [
    "def conv_layer(in_fmap, kernel, bias, act_fun=None, name=None):\n",
    "    with tf.name_scope(name, 'conv'):\n",
    "        tf_kernel = tf.Variable(kernel,\n",
    "                                dtype=tf.float32,\n",
    "                                name='kernel')\n",
    "        tf_bias = tf.Variable(bias,\n",
    "                              dtype=tf.float32,\n",
    "                              name='bias')\n",
    "        out_fmap = tf.add(tf.nn.conv2d(in_fmap, kernel,\n",
    "                                       strides=[1, 1, 1, 1],\n",
    "                                       padding='SAME'),\n",
    "                          tf_bias,\n",
    "                          name='fmap')\n",
    "        if act_fun:\n",
    "            out_fmap = act_fun(out_fmap, name='activation')\n",
    "        return out_fmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:13:40.585181Z",
     "start_time": "2018-08-09T13:13:40.576661Z"
    }
   },
   "outputs": [],
   "source": [
    "def fc_layer(in_fmap, weight, bias, act_fun=None, name=None):\n",
    "    with tf.name_scope(name, 'fc'):\n",
    "        tf_weight = tf.Variable(weight,\n",
    "                                dtype=tf.float32,\n",
    "                                name='weight')\n",
    "        tf_bias = tf.Variable(bias,\n",
    "                              dtype=tf.float32,\n",
    "                              name='bias')\n",
    "        logits = tf.add(tf.matmul(in_fmap, tf_weight),\n",
    "                        tf_bias,\n",
    "                        name='logits')\n",
    "        if act_fun:\n",
    "            logits = act_fun(logits, name='activation')\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:13:41.282103Z",
     "start_time": "2018-08-09T13:13:41.275674Z"
    }
   },
   "outputs": [],
   "source": [
    "def cross_entropy_loss(logits, labels, name=None, axis=-1):\n",
    "    '''https://github.com/keras-team/keras/blob/master/keras/backend/tensorflow_backend.py#L3171\n",
    "    '''\n",
    "    with tf.name_scope(name, 'cross_entropy'):\n",
    "        prob = tf.nn.softmax(logits=logits, axis=axis)\n",
    "        prob = tf.clip_by_value(prob, 1e-7, 1-1e-7)\n",
    "        loss = tf.reduce_sum(-labels * tf.log(prob), name='total_loss')\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:13:43.273946Z",
     "start_time": "2018-08-09T13:13:42.234390Z"
    }
   },
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    tf_image_batch = tf.placeholder(tf.float32, shape=[None, 32, 32, 3])\n",
    "    tf_labels = tf.placeholder(tf.float32, shape=[None, 10])\n",
    "    \n",
    "    conv1 = conv_layer(tf_image_batch,\n",
    "                       pretrain_weights['conv2d']['conv2d/kernel:0'],\n",
    "                       pretrain_weights['conv2d']['conv2d/bias:0'],\n",
    "                       act_fun=tf.nn.relu,\n",
    "                       name='conv1')\n",
    "    conv2 = conv_layer(conv1,\n",
    "                       pretrain_weights['conv2d_1']['conv2d_1/kernel:0'],\n",
    "                       pretrain_weights['conv2d_1']['conv2d_1/bias:0'],\n",
    "                       act_fun=tf.nn.relu,\n",
    "                       name='conv2')\n",
    "    pool1 = tf.nn.max_pool(conv2,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='VALID')\n",
    "    conv3 = conv_layer(pool1,\n",
    "                       pretrain_weights['conv2d_2']['conv2d_2/kernel:0'],\n",
    "                       pretrain_weights['conv2d_2']['conv2d_2/bias:0'],\n",
    "                       act_fun=tf.nn.relu)\n",
    "    pool2 = tf.nn.max_pool(conv3,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='VALID')\n",
    "    conv4 = conv_layer(pool2,\n",
    "                       pretrain_weights['conv2d_3']['conv2d_3/kernel:0'],\n",
    "                       pretrain_weights['conv2d_3']['conv2d_3/bias:0'],\n",
    "                       act_fun=tf.nn.relu)\n",
    "    pool3 = tf.nn.max_pool(conv4,\n",
    "                           ksize=[1, 2, 2, 1],\n",
    "                           strides=[1, 2, 2, 1],\n",
    "                           padding='VALID')\n",
    "    flat_pool3 = tf.reshape(pool3,\n",
    "                            [-1, reduce(lambda x, y: x*y,\n",
    "                                        pool3.shape.as_list()[1:],\n",
    "                                        1)])\n",
    "    tf_keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    dropout1 = tf.nn.dropout(flat_pool3,\n",
    "                             keep_prob=tf_keep_prob,\n",
    "                             name='dropout1')\n",
    "    fc1 = fc_layer(dropout1,\n",
    "                   pretrain_weights['dense']['dense/kernel:0'],\n",
    "                   pretrain_weights['dense']['dense/bias:0'],\n",
    "                   act_fun=tf.nn.relu,\n",
    "                   name='fc1')\n",
    "    dropout2 = tf.nn.dropout(fc1,\n",
    "                             keep_prob=tf_keep_prob,\n",
    "                             name='dropout2')\n",
    "    logits = fc_layer(dropout2,\n",
    "                      pretrain_weights['dense_1']['dense_1/kernel:0'],\n",
    "                      pretrain_weights['dense_1']['dense_1/bias:0'],\n",
    "                      name='logits')\n",
    "    pred_labels = tf.argmax(tf.nn.softmax(logits),\n",
    "                            axis=-1,\n",
    "                            name='pred_labels')\n",
    "    total_loss = cross_entropy_loss(logits=logits, labels=tf_labels)\n",
    "    \n",
    "    train_op = tf.train.AdadeltaOptimizer(learning_rate=1.0, epsilon=1e-7).minimize(total_loss)\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:03:21.888621Z",
     "start_time": "2018-08-09T13:03:21.879329Z"
    }
   },
   "outputs": [],
   "source": [
    "from cifar import read_data_sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:03:23.487165Z",
     "start_time": "2018-08-09T13:03:23.477675Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:03:24.023747Z",
     "start_time": "2018-08-09T13:03:24.007527Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T13:03:24.529978Z",
     "start_time": "2018-08-09T13:03:24.508794Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_iter_per_epoch = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-08-09T13:07:17.763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar data directory found ./data/cifar-10-batches-py\n",
      "loading data...\n"
     ]
    }
   ],
   "source": [
    "!rm -rf ckpt && mkdir -p ckpt/cnn\n",
    "\n",
    "# this will takes long to complete if running on CPU\n",
    "cifar = read_data_sets('./data', one_hot=True, reshape=False)\n",
    "img_gen = ImageDataGenerator(width_shift_range=0.1,\n",
    "                             height_shift_range=0.1,\n",
    "                             horizontal_flip=True)\n",
    "img_gen.fit(cifar.train.images)\n",
    "batch_gen = img_gen.flow(cifar.train.images,\n",
    "                         cifar.train.labels,\n",
    "                         batch_size=batch_size)\n",
    "\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    log_file = sys.stdout# open('train_cnn.log', 'w')\n",
    "    tf.global_variables_initializer().run()\n",
    "    # compute original loss\n",
    "    l, p_labels = sess.run([total_loss, pred_labels],\n",
    "                           feed_dict={tf_image_batch: cifar.train.images,\n",
    "                                      tf_labels: cifar.train.labels,\n",
    "                                      tf_keep_prob: 1.0})\n",
    "    l /= cifar.train.images.shape[0]\n",
    "    acc = (p_labels == np.argmax(cifar.train.labels, axis=-1)).mean()\n",
    "    print(f'original loss: {l}', file=log_file)\n",
    "    print(f'acc: {acc*100:.2f}%', file=log_file)\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        print(f'epoch {epoch} start', file=log_file)\n",
    "        for _ in range(num_iter_per_epoch):\n",
    "            images_batch, labels_batch = next(batch_gen)\n",
    "            _ = sess.run(train_op,\n",
    "                         feed_dict={tf_image_batch: images_batch,\n",
    "                                    tf_labels: labels_batch,\n",
    "                                    tf_keep_prob: 0.8})\n",
    "        train_loss, p_labels = sess.run([total_loss, pred_labels],\n",
    "                                        feed_dict={tf_image_batch: cifar.train.images,\n",
    "                                                   tf_labels: cifar.train.labels,\n",
    "                                                   tf_keep_prob: 1.0})\n",
    "        train_loss /= cifar.train.images.shape[0]\n",
    "        acc = (p_labels == np.argmax(cifar.train.labels)).mean()\n",
    "        print(f'loss: {train_loss}, {acc*100:0.2f}%', file=log_file)\n",
    "        ckpt = saver.save(sess, 'ckpt/cnn/model')\n",
    "        print(f'epoch saved {ckpt}', file=log_file, flush=True)\n",
    "\n",
    "    if log_file is not sys.stdout:\n",
    "        log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T04:37:37.829783Z",
     "start_time": "2018-08-09T02:59:56.461Z"
    }
   },
   "outputs": [],
   "source": [
    "ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-09T04:37:37.831518Z",
     "start_time": "2018-08-09T02:59:57.273Z"
    }
   },
   "outputs": [],
   "source": [
    "!tree ckpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T10:39:17.446893Z",
     "start_time": "2018-08-08T10:38:26.366Z"
    }
   },
   "outputs": [],
   "source": [
    "graph_def = prepare_meta_graph(ckpt+'.meta', output_nodes=[pred_label.op.name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-08T10:39:17.448641Z",
     "start_time": "2018-08-08T10:38:27.604Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('cifar10_cnn.pb', 'wb') as fid:\n",
    "    fid.write(graph_def.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "utensor",
   "language": "python",
   "name": "utensor"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {},
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
